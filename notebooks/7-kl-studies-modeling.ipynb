{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf2e3b7",
   "metadata": {},
   "source": [
    "# <font color='violet'> Modeling to Predict Ratings based on Reviews \n",
    "    \n",
    "Using data with most features engineered here: https://github.com/fractaldatalearning/psychedelic_efficacy/blob/main/notebooks/6-kl-studies-finish-preprocess.ipynb\n",
    "\n",
    "Other feature engineering will be completed as part of the modeling pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8f810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba29f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea2a86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31557 entries, 0 to 31556\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         31557 non-null  int64  \n",
      " 1   condition          31557 non-null  object \n",
      " 2   date               31557 non-null  object \n",
      " 3   drug0              31557 non-null  object \n",
      " 4   drug1              31557 non-null  object \n",
      " 5   review_len         31557 non-null  int64  \n",
      " 6   complexity         31557 non-null  float64\n",
      " 7   no_stop_cap_lemm   31557 non-null  object \n",
      " 8   subjectivity       31557 non-null  float64\n",
      " 9   original_polarity  31557 non-null  float64\n",
      " 10  set                31557 non-null  object \n",
      " 11  rating             31557 non-null  float64\n",
      " 12  similarity_w_10    31557 non-null  float64\n",
      "dtypes: float64(5), int64(2), object(6)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/studies_w_vector_similarity.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb3d421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_len</th>\n",
       "      <th>complexity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>original_polarity</th>\n",
       "      <th>similarity_w_10</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good give run gas</td>\n",
       "      <td>9.0</td>\n",
       "      <td>36</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.640050</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75 mg x daily no noticeable effect 150 mg x da...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>547</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.343056</td>\n",
       "      <td>0.031439</td>\n",
       "      <td>0.841263</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>take 145 mg 10 year fantastic insomnia really ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>390</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>0.922576</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>help stability mood help insomnia start experi...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>156</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.825203</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crazy eat sleep sit</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.467668</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating  review_len  \\\n",
       "0                                  good give run gas     9.0          36   \n",
       "1  75 mg x daily no noticeable effect 150 mg x da...     8.0         547   \n",
       "2  take 145 mg 10 year fantastic insomnia really ...     8.0         390   \n",
       "3  help stability mood help insomnia start experi...     7.0         156   \n",
       "4                                crazy eat sleep sit     2.0          66   \n",
       "\n",
       "   complexity  subjectivity  original_polarity  similarity_w_10    set  \n",
       "0        -1.2      0.600000           0.700000         0.640050  train  \n",
       "1         5.4      0.343056           0.031439         0.841263  train  \n",
       "2         4.8      0.591667           0.096296         0.922576  train  \n",
       "3         8.2      1.000000          -1.000000         0.825203  train  \n",
       "4        -0.4      0.900000          -0.600000         0.467668  train  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove categorical features that aren't specifically related to the review text\n",
    "df = df.drop(columns = ['Unnamed: 0', 'condition', 'date', 'drug0', 'drug1'])\n",
    "\n",
    "# Rename the review column for clarity.\n",
    "df = df.rename(columns={'no_stop_cap_lemm':'review'})\n",
    "\n",
    "# Reorder columns for clarity\n",
    "df = df[['review', 'rating', 'review_len', 'complexity', 'subjectivity', \n",
    "              'original_polarity', 'similarity_w_10', 'set']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d59d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22089 entries, 0 to 22088\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   review             22089 non-null  object \n",
      " 1   rating             22089 non-null  float64\n",
      " 2   review_len         22089 non-null  int64  \n",
      " 3   complexity         22089 non-null  float64\n",
      " 4   subjectivity       22089 non-null  float64\n",
      " 5   original_polarity  22089 non-null  float64\n",
      " 6   similarity_w_10    22089 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train_set = df[df.set=='train'].drop(columns=['set']).copy()\n",
    "test_set = df[df.set=='test'].drop(columns=['set']).copy()\n",
    "\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6f6e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22089 entries, 0 to 22088\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   review             22089 non-null  object \n",
      " 1   review_len         22089 non-null  int64  \n",
      " 2   complexity         22089 non-null  float64\n",
      " 3   subjectivity       22089 non-null  float64\n",
      " 4   original_polarity  22089 non-null  float64\n",
      " 5   similarity_w_10    22089 non-null  float64\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train = train_set.drop(columns=['rating'])\n",
    "X_test = test_set.drop(columns=['rating'])\n",
    "y_train = train_set.rating\n",
    "y_test = test_set.rating\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b05e40",
   "metadata": {},
   "source": [
    "Prepare to normalize numeric columns and use CountVectorizer with review column. I know that most of this data is not normally distributed, so I'll normalize with MinMaxScaler. \n",
    "\n",
    "Prepare to try out PCA on just the numeric columns after scaling them. \n",
    "\n",
    "Finally, prepare to randomly search parameters for the various classifiers I've imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95d18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/psychedelic_efficacy/lib/python3.9/site-packages/sklearn/model_selection/_search.py:306: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "review_col = 0\n",
    "numeric_cols = [1,2,3,4,5]\n",
    "\n",
    "\n",
    "# Normalize numeric columns first, then do PCA on numeric columns, then vectorize text. \n",
    "# Try performing PCA with a few different component values or no reduction at all. \n",
    "# Use columntransformer in order to do pca with numeric columns only, not vectorized text\n",
    "pca1 = ColumnTransformer([('mms', MinMaxScaler(), numeric_cols), \n",
    "                          ('pca1', PCA(n_components=1, random_state=17), numeric_cols), \n",
    "                          ('cv', CountVectorizer(lowercase=False), review_col)])\n",
    "pca2 = ColumnTransformer([('mms', MinMaxScaler(), numeric_cols), \n",
    "                          ('pca2', PCA(n_components=2, random_state=17), numeric_cols), \n",
    "                          ('cv', CountVectorizer(lowercase=False), review_col)])\n",
    "pca3 = ColumnTransformer([('mms', MinMaxScaler(), numeric_cols), \n",
    "                          ('pca1', PCA(n_components=1, random_state=17), numeric_cols), \n",
    "                          ('cv', CountVectorizer(lowercase=False), review_col)])\n",
    "pca4 = ColumnTransformer([('mms', MinMaxScaler(), numeric_cols), \n",
    "                          ('cv', CountVectorizer(lowercase=False), review_col)])\n",
    "pca_params = {'pca':[pca1, pca2, pca3, pca4]}\n",
    "\n",
    "# Compile gridsearch parameters and pipeline. Run through randomized grid search\n",
    "\n",
    "pipe = Pipeline(steps=[('pca', pca1), ('clf', SVC(probability=True))])\n",
    "\n",
    "rgs = RandomizedSearchCV(estimator=pipe, param_distributions=pca_params, \n",
    "                             scoring='roc_auc_ovr', random_state=17, error_score='raise')\n",
    "\n",
    "\n",
    "# Run the gridsearch\n",
    "rgs.fit(X_train, y_train)\n",
    "print(rgs.best_params_)\n",
    "print(rgs.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b40a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full with bugs; trying a portion above to fix early bugs.\n",
    "\n",
    "review_col = 0\n",
    "numeric_cols = [1,2,3,4,5]\n",
    "\n",
    "\n",
    "# Normalize numeric columns first. \n",
    "# Try performing PCA with a few different component values or no reduction at all. \n",
    "# Use columntransformer in order to do pca with numeric columns only, not vectorized text\n",
    "pca1 = ColumnTransformer([('mms', MinMaxScaler(), numeric_cols), \n",
    "                          ('pca1', PCA(n_components=1, random_state=17), numeric_cols)], \n",
    "                         remainder='passthrough')\n",
    "pca2 = ColumnTransformer([('mms', MinMaxScaler(), numeric_cols), \n",
    "                          ('pca1', PCA(n_components=2, random_state=17), numeric_cols)], \n",
    "                         remainder='passthrough')\n",
    "pca3 = ColumnTransformer([('mms', MinMaxScaler(), numeric_cols), \n",
    "                          ('pca1', PCA(n_components=3, random_state=17), numeric_cols)], \n",
    "                         remainder='passthrough')\n",
    "pca_params = {'pca':[pca1, pca2, pca3, 'passthrough']}\n",
    "\n",
    "# Vectorize text data\n",
    "text_trans = ColumnTransformer([('cv', CountVectorizer(lowercase=False), review_col)], \n",
    "                              remainder='passthrough')\n",
    "\n",
    "\n",
    "# Set up classifier hyperparameters to tune  \n",
    "\n",
    "clf1 = KNeighborsClassifier()\n",
    "param1 = dict(clf=(clf1,), clf__n_neighbors=list(np.arange(3,22,2)), \n",
    "              clf__weights=['uniform','distance'],\n",
    "              clf__leaf_size=list(np.arange(10,101,10)), \n",
    "              clf__p=[1,2], clf__metric=['euclidean','chebyshev','minkowski'])\n",
    "\n",
    "clf2 = SVC(probability=True, random_state=43)\n",
    "param2 = dict(clf=(clf2,), clf__C=list(np.arange(1,11)), \n",
    "              clf__kernel=['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              clf__degree=list(np.arange(1,11)), clf__gamma=['scale', 'auto'], \n",
    "              clf__coef0=list(np.arange(0,4,0.5)), clf__shrinking=[True,False], \n",
    "              clf__probability=[True,False], clf__class_weight=[None,'balanced'])\n",
    "\n",
    "# Multinomial NB requires positve values. Find a different way to try beysian classifier\n",
    "clf3 = MultinomialNB()\n",
    "param3 = dict(clf=(clf3,), clf__fit_prior=[True,False])\n",
    "\n",
    "# Only select potential hyperparameters that I've read minimize overfitting\n",
    "clf4 = XGBClassifier()\n",
    "param4 = dict(clf=(clf4,), clf__colsample_bytree=list(np.arange(0, 0.6, 0.1)), \n",
    "              clf__subsample=list(np.arange(0, 0.6, 0.1)), \n",
    "              clf__max_depth=list(np.arange(1,5)), clf__gamma=list(np.arange(4,11,1)), \n",
    "              clf__eta=list(np.arange(0, 0.6, 0.1)), clf__min_child_weight=[5,20,50,100,200], \n",
    "              clf__alpha=[5,10,20,50,100], clf__n_estimators=[5,10,20,50])\n",
    "\n",
    "clf5 = RidgeClassifier(class_weight='balanced', random_state=17)\n",
    "param5 = dict(clf=(clf5,), clf__alpha=[1e-10, 1e-5, 1e-2, 1, 5, 10, 20, 50, 100, 200, 500])\n",
    "\n",
    "clf6 = LinearDiscriminantAnalysis(shrinkage='auto')\n",
    "param6 = dict(clf=(clf6,), clf__solver=['svd', 'lsqr', 'eigen'], \n",
    "              clf__store_covariance=[True,False])\n",
    "\n",
    "# Compile gridsearch parameters and pipeline. Run through randomized grid search\n",
    "\n",
    "pipe = Pipeline(steps=[('pca', pca1), ('text', text_trans), ('clf', clf1)])\n",
    "\n",
    "param_grid = [pca_params, param1, param2, param3, param4, param5, param6]\n",
    "\n",
    "rgs = RandomizedSearchCV(estimator=pipe, param_distributions=param_grid, \n",
    "                             scoring='roc_auc', random_state=17, error_score='raise')\n",
    "\n",
    "\n",
    "# Run the gridsearch\n",
    "rgs.fit(X_train, y_train)\n",
    "print(rgs.best_params_)\n",
    "print(rgs.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bfc72",
   "metadata": {},
   "source": [
    "Note: I drew some code from this resource: https://towardsdatascience.com/getting-the-most-out-of-scikit-learn-pipelines-c2afc4410f1a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
