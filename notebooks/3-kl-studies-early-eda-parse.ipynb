{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2d0283",
   "metadata": {},
   "source": [
    "# <font color='violet'> Exploration & Parsing\n",
    "Using prescription drug review data wrangled here: https://github.com/fractaldatalearning/psychedelic_efficacy/blob/main/notebooks/1-kl-wrangle-tabular.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb2ba9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tqdm \n",
    "# !{sys.executable} -m pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259f1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import contractions\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9731c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to add local python functions; import modules from src directory\n",
    "src = '../src'\n",
    "sys.path.append(src)\n",
    "\n",
    "# import local functions\n",
    "from nlp.parse import remove_accented_chars, strip_most_punc, strip_apostrophe, \\\n",
    "strip_non_emoji_emoji_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1fb33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31554 entries, 0 to 31553\n",
      "Data columns (total 21 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  31554 non-null  int64  \n",
      " 1   rating      31554 non-null  float64\n",
      " 2   condition   31554 non-null  object \n",
      " 3   review      31554 non-null  object \n",
      " 4   date        31554 non-null  object \n",
      " 5   drug0       31554 non-null  object \n",
      " 6   drug1       18992 non-null  object \n",
      " 7   drug2       32 non-null     object \n",
      " 8   drug3       23 non-null     object \n",
      " 9   drug4       12 non-null     object \n",
      " 10  drug5       11 non-null     object \n",
      " 11  drug6       7 non-null      object \n",
      " 12  drug7       5 non-null      object \n",
      " 13  drug8       3 non-null      object \n",
      " 14  drug9       2 non-null      object \n",
      " 15  drug10      2 non-null      object \n",
      " 16  drug11      2 non-null      object \n",
      " 17  drug12      2 non-null      object \n",
      " 18  drug13      2 non-null      object \n",
      " 19  drug14      1 non-null      object \n",
      " 20  drug15      1 non-null      object \n",
      "dtypes: float64(1), int64(1), object(19)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/studies_no_duplicates.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "befb84a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "      <th>drug0</th>\n",
       "      <th>drug1</th>\n",
       "      <th>drug2</th>\n",
       "      <th>drug3</th>\n",
       "      <th>drug4</th>\n",
       "      <th>drug5</th>\n",
       "      <th>drug6</th>\n",
       "      <th>drug7</th>\n",
       "      <th>drug8</th>\n",
       "      <th>drug9</th>\n",
       "      <th>drug10</th>\n",
       "      <th>drug11</th>\n",
       "      <th>drug12</th>\n",
       "      <th>drug13</th>\n",
       "      <th>drug14</th>\n",
       "      <th>drug15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>adhd</td>\n",
       "      <td>I have only been on Vyvanse for 2 weeks.  I st...</td>\n",
       "      <td>0</td>\n",
       "      <td>vyvanse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>add</td>\n",
       "      <td>So far the throwing up has stopped and the hea...</td>\n",
       "      <td>0</td>\n",
       "      <td>vyvanse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating condition                                             review date  \\\n",
       "0     8.0      adhd  I have only been on Vyvanse for 2 weeks.  I st...    0   \n",
       "1     7.0       add  So far the throwing up has stopped and the hea...    0   \n",
       "\n",
       "     drug0 drug1 drug2 drug3 drug4 drug5 drug6 drug7 drug8 drug9 drug10  \\\n",
       "0  vyvanse   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN    NaN   \n",
       "1  vyvanse   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN    NaN   \n",
       "\n",
       "  drug11 drug12 drug13 drug14 drug15  \n",
       "0    NaN    NaN    NaN    NaN    NaN  \n",
       "1    NaN    NaN    NaN    NaN    NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop \"Unnamed\" column; it's redundant with the index\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6003f8f2",
   "metadata": {},
   "source": [
    "<font color='violet'> Explore each column, starting with drug columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efae90ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drug0 holds the name of the most-commonly reviewed drug reviewed for a particular row\n",
    "# How many are there?\n",
    "len(df.drug0.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f85e271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['vyvanse', 'ritalin-la', 'wellbutrin-sr', 'concerta', 'strattera',\n",
       "       'ritalin', 'adderall', 'diazepam', 'pristiq', 'dextrostat',\n",
       "       'clonazepam', 'provigil', 'baclofen', 'dexedrine', 'adderall-xr',\n",
       "       'dextroamphetamine', 'chantix', 'lorazepam', 'amphetamine',\n",
       "       'methadone', 'wellbutrin-xl', 'focalin-xr', 'methylphenidate',\n",
       "       'ativan', 'effexor', 'Mirtazapine', 'Methadone', 'Quetiapine',\n",
       "       'Zolpidem', 'Varenicline', 'Clonazepam', 'Trazodone',\n",
       "       'Aripiprazole', 'Lurasidone', 'Lamotrigine', 'Escitalopram',\n",
       "       'Acamprosate', 'Gabapentin', 'Bupropion', 'Venlafaxine',\n",
       "       'Sertraline', 'Pregabalin', 'Buspirone', 'Nicotine',\n",
       "       'Divalproex sodium', 'Fluoxetine', 'Desvenlafaxine',\n",
       "       'Lisdexamfetamine', 'Buprenorphine / naloxone', 'Temazepam',\n",
       "       'Diazepam', 'Drospirenone / ethinyl estradiol', 'Vilazodone',\n",
       "       'Caffeine', 'Fluoxetine / olanzapine', 'Tranexamic acid',\n",
       "       'Nefazodone', 'Diphenhydramine', 'Paroxetine', 'Vortioxetine',\n",
       "       'Alprazolam', 'Citalopram', 'Ethinyl estradiol / norethindrone',\n",
       "       'Risperidone', 'Amitriptyline', 'Chlordiazepoxide', 'Atomoxetine',\n",
       "       'Tramadol', 'Levomilnacipran', 'Lorazepam', 'Asenapine',\n",
       "       'Naltrexone', 'Suvorexant', 'Prazosin', 'Hydrocodone',\n",
       "       'Armodafinil', 'Donepezil', 'Acetaminophen / diphenhydramine',\n",
       "       'Duloxetine', 'Hydroxyzine', 'Eszopiclone', 'Ramelteon', 'Doxepin',\n",
       "       'Ziprasidone', 'Clonidine', 'Fluvoxamine', 'Topiramate',\n",
       "       'Amphetamine / dextroamphetamine', 'Melatonin', 'Niacin',\n",
       "       'Estradiol', 'Olanzapine',\n",
       "       'Drospirenone / ethinyl estradiol / levomefolate calcium',\n",
       "       'Fentanyl', 'Atenolol', 'L-methylfolate', 'Selegiline',\n",
       "       'Oxcarbazepine', 'Sildenafil', 'Kava', 'Cariprazine', 'Midazolam',\n",
       "       'Paliperidone', 'Triazolam', 'Brexpiprazole', 'Itraconazole',\n",
       "       'Histrelin', 'Methocarbamol', 'Chlorpromazine', 'Hetlioz',\n",
       "       'Lithium', 'Trihexyphenidyl', 'Estropipate', 'Buprenorphine',\n",
       "       'Flurazepam', 'Carbamazepine', 'Disulfiram', 'Valproic acid',\n",
       "       'Levetiracetam', 'Phentermine', 'Dexmethylphenidate',\n",
       "       'Cyproheptadine', 'Baclofen', 'Clorazepate',\n",
       "       'Conjugated estrogens', 'Haloperidol', 'Galantamine',\n",
       "       'Diphenhydramine / naproxen', 'Pyridostigmine', 'Clomipramine',\n",
       "       'Secobarbital', 'Perphenazine', \"St. john's wort\",\n",
       "       'Chloral hydrate', 'Doxylamine', 'Hydromorphone', 'Modafinil',\n",
       "       'Guaifenesin / pseudoephedrine', 'Desipramine', 'Tranylcypromine',\n",
       "       'Quazepam', 'Estradiol / norethindrone', 'Propranolol',\n",
       "       'Ethinyl estradiol / levonorgestrel', 'Nortriptyline', 'Zaleplon',\n",
       "       'Phenelzine', 'Testosterone', 'Rivastigmine', 'Angeliq',\n",
       "       'Hydrochlorothiazide / triamterene', 'Phenytoin',\n",
       "       'Dextroamphetamine', 'Imipramine', 'Methylprednisolone',\n",
       "       'Oxazepam', 'Dronabinol', 'Trifluoperazine', 'Trimipramine',\n",
       "       'Naproxen', 'Acetaminophen / pamabrom / pyrilamine',\n",
       "       'Conjugated estrogens / medroxyprogesterone', 'Iloperidone',\n",
       "       'Amoxapine', 'Memantine', 'Oxycodone', 'Apixaban', 'Fluphenazine',\n",
       "       'Meloxicam', 'Cyclobenzaprine', 'Estazolam', 'Isotretinoin',\n",
       "       'Promethazine', 'Clarithromycin', 'Olmesartan', 'Yohimbine',\n",
       "       'Methylphenidate', 'Azithromycin', 'Ketamine',\n",
       "       'Estradiol / levonorgestrel', 'Meperidine', 'Clozapine',\n",
       "       'Lactulose', 'Morphine', 'Amitriptyline / chlordiazepoxide',\n",
       "       'Lidocaine', 'Phenazopyridine', 'Thioridazine', 'Naloxone',\n",
       "       'Progesterone', 'Sucralfate', 'Pamabrom', 'Flibanserin',\n",
       "       'Thiothixene', 'Valerian', 'Dobutamine', 'Prednisone', 'Acyclovir',\n",
       "       'Amitriptyline / perphenazine', 'Prochlorperazine',\n",
       "       'Penicillin v potassium', 'Loxapine', 'Dermal filler', 'Duavee',\n",
       "       'Megestrol', 'Milk thistle', 'Aspirin / meprobamate',\n",
       "       'Rosuvastatin', 'Esterified estrogens / methyltestosterone',\n",
       "       'Esomeprazole', 'Inderal', 'Amantadine', 'Gentian violet',\n",
       "       'Terbinafine', 'Leuprolide', 'Acetaminophen / hydrocodone',\n",
       "       'Tryptophan', 'Acetaminophen / oxycodone', 'Diclofenac',\n",
       "       'Pentobarbital', 'Ethinyl estradiol / norgestimate',\n",
       "       'Medroxyprogesterone', 'Metronidazole', 'Nadolol', 'Azelastine',\n",
       "       'Butabarbital', 'Amoxicillin / clavulanate', 'Levonorgestrel',\n",
       "       'Tamoxifen', 'Brimonidine', 'Betamethasone',\n",
       "       'Evening Primrose Oil', 'Amlodipine / benazepril',\n",
       "       'Ethinyl estradiol / etonogestrel', 'Ranitidine', 'Levofloxacin',\n",
       "       'Hydrocortisone / neomycin / polymyxin b', 'Ketoconazole',\n",
       "       'Ropinirole', 'Flavoxate', 'Clindamycin', 'Orlistat',\n",
       "       'Ethinyl estradiol / norgestrel', 'Meprobamate', 'Mupirocin',\n",
       "       'Nitrofurantoin', 'Ondansetron', 'Ciprofloxacin',\n",
       "       'Acetaminophen / propoxyphene', 'Amlodipine', 'Maprotiline',\n",
       "       'Biltricide', 'Atarax', 'S-adenosylmethionine', 'Alfuzosin',\n",
       "       'Guaifenesin', 'Ibuprofen', 'Lisinopril', 'Riluzole',\n",
       "       'Gatifloxacin', 'Indomethacin', 'Magnesium citrate',\n",
       "       'Sodium hyaluronate', 'Isocarboxazid', 'Doxazosin',\n",
       "       'Electrolyte replacement solutions', 'Doxycycline', 'Zolmitriptan',\n",
       "       'Metoprolol', 'Olodaterol', 'Hydrochlorothiazide / telmisartan',\n",
       "       'Irbesartan', 'Aspirin / butalbital / caffeine',\n",
       "       'Acetaminophen / tramadol', 'Esterified estrogens', 'Rizatriptan',\n",
       "       'Undecylenic acid', 'Magnesium oxide', 'Clobetasol', 'Vancomycin',\n",
       "       'Aspirin / diphenhydramine', 'Loratadine', 'Cetirizine',\n",
       "       'Silver sulfadiazine', 'Opium', 'Sumatriptan', 'Levothyroxine',\n",
       "       'Hydroxyurea', 'Acetaminophen / butalbital / caffeine',\n",
       "       'Chlorpheniramine / hydrocodone', 'Etanercept', 'Advil PM',\n",
       "       'Levocetirizine', 'Fluticasone', 'Adalimumab',\n",
       "       'Multivitamin with iron', 'Adenosine', 'Sorafenib', 'Turmeric',\n",
       "       'Nabumetone', 'Acetaminophen / caffeine / pyrilamine', 'Meclizine',\n",
       "       'Sulfamethoxazole / trimethoprim', 'Norethindrone',\n",
       "       'Gabapentin enacarbil', 'Nystatin / triamcinolone', 'Teriparatide',\n",
       "       'Nifedipine', 'Fenofibrate', 'Tadalafil', 'Amoxicillin',\n",
       "       'Desmopressin', 'Misoprostol', 'Acetaminophen / phenylephrine',\n",
       "       'Celecoxib', 'Phendimetrazine', 'Iohexol', 'Spironolactone',\n",
       "       'Molindone', 'Dexamethasone', 'Oxybutynin', 'Propafenone',\n",
       "       'Apremilast', 'Biotin', 'Nebivolol', 'Amikacin',\n",
       "       'Thyroid desiccated', 'Camphor / menthol', 'Exenatide',\n",
       "       'Minocycline', 'Indapamide', 'Propofol', 'Dimenhydrinate',\n",
       "       'Balsalazide', 'Hydrochlorothiazide / olmesartan',\n",
       "       'Efavirenz / emtricitabine / tenofovir', 'Vivactil', 'Liraglutide',\n",
       "       'Hydroxychloroquine', 'Benzoyl peroxide', 'Fexofenadine',\n",
       "       'Mefenamic acid', 'Cholestyramine', 'Atropine',\n",
       "       'Hydrochlorothiazide / losartan', 'Minoxidil', 'Creatine',\n",
       "       'Bacitracin / neomycin / polymyxin b', 'Phenobarbital',\n",
       "       'Metoclopramide', 'Pramipexole', 'Pseudoephedrine', 'Carvedilol',\n",
       "       'Dasatinib', 'Diltiazem', 'Losartan', 'Bethanechol',\n",
       "       'Polyethylene glycol 3350 with electrolytes', 'Mometasone',\n",
       "       'Methscopolamine', 'Glipizide', 'Tizanidine',\n",
       "       'Fexofenadine / pseudoephedrine', 'Tiagabine', 'Triamcinolone',\n",
       "       'Dimethyl sulfoxide', 'Diethylpropion', 'Capsaicin',\n",
       "       'Trandolapril / verapamil', 'Regadenoson',\n",
       "       'Magnesium sulfate / potassium sulfate / sodium sulfate',\n",
       "       'Ethchlorvynol', 'Carisoprodol'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many total drugs are there?\n",
    "df.drug0.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f117deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which drugs are most commonly reviewed?\n",
    "freq_drugs = df.drug.value_counts().head(10)\n",
    "freq_drugs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880f582",
   "metadata": {},
   "source": [
    "It would be best to add a 'drug class' column when I come to feature engineering so that all these drugs are categorized. That column existed previously but came from one of the origina tables where too few of the rows had reviews for psych meds. I could eventually do this by creating a dictionary of drugs and their classes using information scraped from this website: https://www.drugs.com/drug-classes.html\n",
    "\n",
    "Alternatively, drugs could be understood by the conditions they treat. \n",
    "\n",
    "<font color='violet'> Explore conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.condition.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f967ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_conditions = df.condition.value_counts().head(10)\n",
    "freq_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which of the most common drugs are used to treat which of the most common conditions?\n",
    "\n",
    "freq_drugs = ['Sertraline', 'Escitalopram', 'Citalopram', 'Bupropion', 'Lexapro', \n",
    "             'Venlafaxine', 'Varenicline', 'Zoloft', 'Quetiapine', 'Clonazepam']\n",
    "freq_conditions = ['depression', 'anxiety', 'bipolar', 'addiction', 'insomnia', 'hrt',\n",
    "                  'schizophrenia', 'ocd', 'other', 'schizoaffective disorder']\n",
    "freq_drug_conditions = df[df['drug'].isin(freq_drugs) & df['condition'].isin(freq_conditions)]\n",
    "\n",
    "freq_drug_conditions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_combo_summary = freq_drug_conditions.pivot_table(index='condition', columns='drug', \n",
    "                                                    aggfunc='count', values='review')\n",
    "freq_combo_summary.columns = freq_drugs\n",
    "freq_combo_summary = freq_combo_summary.sort_values(by=freq_drugs, ascending=False)\n",
    "freq_combo_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of reviews across common conditions & drugs with a heatmap.\n",
    "sns.heatmap(freq_combo_summary, cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625b56d",
   "metadata": {},
   "source": [
    "<font color='violet'> Which drugs & conditions have the highest ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_drugs = df.groupby(['drug'])['rating'].mean().sort_values(ascending=False)\n",
    "top_drugs.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7747088",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_conditions = df.groupby(['condition'])['rating'].mean().sort_values(\n",
    "    ascending=False)\n",
    "successful_conditions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_combo = df.groupby(['drug', 'condition'])['rating'].mean().sort_values(\n",
    "    ascending=False)\n",
    "top_combo.head(140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_freq_drugs = set(freq_drugs).intersection(set(top_drugs.index[0:79]))\n",
    "top_freq_drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_freq_drugs_by_condition = set(freq_drugs).intersection(set(top_combo.index[0:138]))\n",
    "top_freq_drugs_by_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_freq_conditions = set(freq_conditions).intersection(set(\n",
    "    successful_conditions.index[0:10]))\n",
    "successful_freq_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452395b9",
   "metadata": {},
   "source": [
    "Anxiety, addction, and ocd are conditions for which there are many drug reviews and high rates of success with treatment. \n",
    "\n",
    "The 10 most frequently-reviewed drugs have nothing in common with the 79 perfectly-rated drugs or the 138 drugs that are rated perfectly for any single condition. My hypothesis is that these drugs may have only one or very few reviews each, which is how their average rating is so high. \n",
    "\n",
    "<font color='violet'> Explore distribution of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b911546",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.rating)\n",
    "plt.axvline(df.rating.mean(), color='orange')\n",
    "plt.axvline(df.rating.median(), color='violet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d768d",
   "metadata": {},
   "source": [
    "More participants gave their drug a high review than gave low reviews, and even fewer gave mediocre reviews.\n",
    "\n",
    "Now, find out: of drugs that received an average rating of 10, how many reiews is that mean derived from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_avg_rating = set(top_drugs.index[0:79])\n",
    "df[df.drug.isin(perfect_avg_rating)].value_counts(subset='drug')[0:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aea20a",
   "metadata": {},
   "source": [
    "Of the 79 drugs with perfect average ratings, only 16 of them had more than one rating, and only 3 of them had more than 3 ratings. Given that there are about 50500 ratings and 650 drugs, the average number of ratings per drug is about 80, so the perfectly-rated drugs definitely seem like outliers. I'd not be surprised if a model eventually has a difficult time correctly classifying the extreme ratings, but for now I'll just keep this in mind and see what happens. \n",
    "\n",
    "<font color='violet'> What does the distribution of ratings look like for drugs with at least 20 ratings (20 = 25% of the average number of ratings)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9fd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ratings_count'] = df.groupby(['drug'])['drug'].transform('count')\n",
    "df.sort_values('ratings_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf71b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_plus_ratings = df[df.ratings_count>=20]\n",
    "twenty_plus_ratings.sort_values('ratings_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(twenty_plus_ratings.rating)\n",
    "plt.axvline(df.rating.mean(), color='orange')\n",
    "plt.axvline(df.rating.median(), color='violet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94b483",
   "metadata": {},
   "source": [
    "This distribution doesn't look much different from that which includes all reviews, which tells me the outliers aren't affecting the distribution too much. So it's probably a good idea to keep all rows in the dataset when moving forward.  \n",
    "\n",
    "<font color='violet'> What is the relationship between date and reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b432c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = df.date.replace('0', np.nan)\n",
    "df.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = pd.to_datetime(df.date)\n",
    "df.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb94bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there more or fewer reviews from any given point in time?\n",
    "df['count_by_date'] = df.groupby(['date'])['date'].transform('count')\n",
    "unique_dates = df.drop_duplicates(subset=['date'])\n",
    "unique_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fe8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002cfefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=unique_dates, x='date', y='count_by_date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2864abf",
   "metadata": {},
   "source": [
    "There was an increase in the number of reviews submitted daily around 2015. \n",
    "\n",
    "<font color='violet'> Do ratings change with time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df, x='date', y='rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86cd76",
   "metadata": {},
   "source": [
    "This looks like something other than total random noise, like maybe there were some current events happening around 2009 and again in 2015 that led people to start rating their psych meds less favorably. There may also be some annual seaonality. Whatever the reason, it seems that date could be correlated with rating and should not be removed. Process this column further to better understand the relationship between date and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_date = df[['date', 'rating']].dropna().set_index('date')\n",
    "rating_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26fb701",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_week = rating_date.resample('W').mean()\n",
    "downsample_week.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624cdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mean = downsample_week.rolling(window=30).mean()\n",
    "rolling_mean.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd89f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for seasonality\n",
    "index_month = rating_date.index.month\n",
    "rating_by_month = rating_date.groupby(index_month).mean()\n",
    "rating_by_month.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33f9836",
   "metadata": {},
   "source": [
    "It seems that seasonal variation is less extreme than variation by year (average range of 7.3-7.55 instead of 6.5-9.0), with people rating their drugs as being, on average, very slightly less effective in July-November. \n",
    "\n",
    "It is even more clear now that weekly average ratings of drugs in these studies did in facat dip in 2009 and again in 2015. The purpose behind these trends isn't so important (though I have some guesses as to what was happening in 2009 and 2015). The date, though, will be a valuable variable alongside narrataive text features when predicting ratings, so as to compare like with like current-events wise. \n",
    "\n",
    "Now, move from the more quantitative data into the narrative column, cleaning up language therein. \n",
    "\n",
    "<font color='violet'> Parse Language\n",
    "    \n",
    "The review column contains narratives where patients explain their experience with a prescription psych med. Language features from that column need to be extracted or created after any necessary cleaning of strings has been completed. Do any preparations necessary to conduct sentiment analysis. I'll be drawing quite a bit from the following resource: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46517f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample string. Search for special characters.\n",
    "df.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['review'].str.find(\"é\")!=-1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaaea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['review'].str.find(\"ä\")!=-1].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48a3bb",
   "metadata": {},
   "source": [
    "<font color='violet'> Remove Most Special Characters\n",
    "\n",
    "...if there are any. Haven't been able to find any of the most common special characters é or ä in the data, but doing it just in case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b71dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function works in the test suite, but there may not be examples in the data\n",
    "df['review'] = df['review'].apply(remove_accented_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae25ec35",
   "metadata": {},
   "source": [
    "<font color='violet'> Expand Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, find some to confirm it works. \n",
    "df[df['review'].str.find(\"'\")!=-1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f946b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e1fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(contractions.fix)\n",
    "df.review[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1f4d1",
   "metadata": {},
   "source": [
    "\"Don't\" got changed to \"do not\"; contraction expansion worked. \n",
    "\n",
    "<font color='violet'> Remove punctuation/special characters where appropriate. \n",
    "    \n",
    "Try to keep those correlated with sentiment: ! ? # % ;) :( .  Again, first find an example to confirm it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['review'].str.find(\"!\")!=-1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cf3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d66da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use function from package I made to get rid of most of the punctuation.\n",
    "strip_most_punc(df, 'review')\n",
    "df.review[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left to strip are ' and : ; () where they don't appear as emoji.\n",
    "\n",
    "df[df['review'].str.find(\"'\")!=-1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d2f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_apostrophe(df, 'review')\n",
    "df.review[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d70245",
   "metadata": {},
   "source": [
    "Stripping apostrophes worked. \n",
    "\n",
    "<font color='violet'> Zoom in on characters that are commonly used in emoji and remove them where they don't appear as part of an emoticon. \n",
    "    \n",
    "Now remove :;() when they appear next to a letter, not emoji. This isn't a perfect solution, as many characters that I already removed can get used in emoji, but at least the most common emoji will be preserved. I'm not going to search for places where these appear next to numbers because my assumption is that symbols appear next to numbers more often as emoji, compared with letters which appear more often next to symbols used for basic punctuation. \n",
    "    \n",
    "Row 6 from earlier has an emoji ;) as well as other ( and ) symbols. Where might I find some other : and ; to see if I'm successfully removing them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ba89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['review'].str.find(\":\")!=-1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That example has lots of weird stuff going on; deal with that eventually if necessary.\n",
    "# For now I can at lease see where the : is (row 2) and check if my function below delets it\n",
    "# Finally, find an example of ;\n",
    "\n",
    "df[df['review'].str.find(\";\")!=-1].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yikes, I'm glad I discovered those duplicate reviews; deal with that shortly\n",
    "df.review[78]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d000f23",
   "metadata": {},
   "source": [
    "<font color='violet'> This was working several times, and I changed nothing but it stopped working. Debug later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the ; in that last example above shows up in the third to last row.\n",
    "# Use a function that can remove these characters appropriately\n",
    "strip_non_emoji_emoji_symbol(df,'review')\n",
    "\n",
    "df.review[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26036882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7dfddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ;) is intact, but all other parentheses that had been in the string from row 6 are gone!\n",
    "# Check on the two other strings.\n",
    "df.review[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d54d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review[78]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f0d34",
   "metadata": {},
   "source": [
    "Symbol removal so far has worked well. I'm going to stop with that becaue I don't need perfection, just cleaner text than I started with so as to end up with fewer oddballs to deal with if I want to do something like making a bag of words. \n",
    "\n",
    "I do definitely want to figure out if there are a bunch of rows with duplicate reviews. It seems that what I discovered is one person may have just written one big review for all their drugs and entered it multiple times, with a different drug and rating each time. Is this behavior an outlier or are there other examples like this? \n",
    "\n",
    "<font color='violet'> Decide what to do about duplicated reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78254eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.review.duplicated()==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd97283",
   "metadata": {},
   "source": [
    "Many rows actually contain duplicate reviews, each connected with multiple different drugs. Did the data start out this way, or did I make an error during wrangling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_dotcom_train = pd.read_csv('../data/raw/drugsComTrain_raw.tsv', sep='\\t')\n",
    "drugs_dotcom_test = pd.read_csv('../data/raw/drugsComTest_raw.tsv', sep='\\t')\n",
    "druglib_train = pd.read_csv('../data/raw/drugLibTrain_raw.tsv', sep='\\t')\n",
    "druglib_test = pd.read_csv('../data/raw/drugLibTest_raw.tsv', sep='\\t')\n",
    "psytar = pd.read_csv('../data/raw/PsyTAR_dataset_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to help figure out what's going on \n",
    "def inspect_duplicate_reviews(df, column):\n",
    "    df = df.sort_values(by=column)\n",
    "    print(len(df), len(df[df[column].duplicated()==True]))\n",
    "    return df[df[column].duplicated()==True].head()\n",
    "\n",
    "# What my current working data looks like\n",
    "inspect_duplicate_reviews(df, 'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out each of the other raw datasets\n",
    "drugs_dotcom_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac61dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_duplicate_reviews(drugs_dotcom_train, 'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7727a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30% of the original reviews from that set were duplicates. \n",
    "inspect_duplicate_reviews(drugs_dotcom_test, 'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1773682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% of drugs_dotcom_test was duplicates\n",
    "druglib_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c33c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_duplicate_reviews(druglib_train, 'commentsReview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89633a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fewer of these were duplicates\n",
    "psytar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_duplicate_reviews(psytar, 'comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adec8c",
   "metadata": {},
   "source": [
    "This last raw dataset has about 15% duplicate values but few rows overall. \n",
    "\n",
    "I did go back to the wrangling notebook and don't see any errors that would have caused this. I think I just didn't notice earlier because I would expect there to be duplicates in many of the columns (drug, condition) without it being a problem at all. Or perhaps completely duplicated rows, and took care of those. But it didn't cross my mind to think that specifically the reveiw column would have duplicates across multiple drugs. \n",
    "\n",
    "There are enough duplicated reviews in the raw data to account for all the duplicates in my current dataframe. My best working hypothesis is that the duplicate reviews appeared more often with psych meds because people may cycle through and try many drugs and then write up one big narrative to submit. Or perhaps, they feel one way about the drug's effects and go back to change their rating later, which results in two rows varying only by rating. I may need to more closely inspect each set of duplicates and find out which drugs the reviews are actually relevant for, removing the rest of the rows. \n",
    "\n",
    "<font color='violet'> Remove rows with irrelevant duplicated reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with just one set of duplicates and see what I find.\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9dab07",
   "metadata": {},
   "source": [
    "It appears that somebody submitted the same review for vyvanse, dextroamphetamine, saizen, and zyprexa. And with vyvanse, they submitted it as being used to treat both add and adhd. And for add they gave it a rating of 9 with one submission and 10 with another. \n",
    "\n",
    "I can see already that this definitly pertains to vyvanse. Since the add ratings are ambiguous, I can just get rid of those and keep the row for adhd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels=[0,5])\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a closer look at the full review to see if it pertains to the other drugs.\n",
    "df.review[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only pertains to vyvanse. Drop other rows. \n",
    "df = df.drop(labels=[1,3,4])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a29fd",
   "metadata": {},
   "source": [
    "How many sets of duplicates will I need to work with? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.review.duplicated()==True]['review'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d03b16",
   "metadata": {},
   "source": [
    "There are so many sets of duplicates, I'm going to need to find some way to automate or otherwise speed up row deletion.\n",
    "\n",
    "This could be a place to group by the review until there's just one row per review with various drug/rating/condition combinations that can be aggregated for each set of duplicates or analyzed more easily in batches for quicker identification of values to keep or delete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I no longer need the ratings counts\n",
    "long_df = df.drop(columns=['ratings_count', 'count_by_date'])\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a columm where I can hold whether each row should be kept or deleted. \n",
    "# Work until every row is filled with a value, then delete indicated rows.\n",
    "long_df['keep'] = ''\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4000b45",
   "metadata": {},
   "source": [
    "<font color='violet'> Mark for keeping any rows where the name of the drug is contained in the text of the review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = long_df.groupby(['review', 'drug']).count()\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row indices are defined by the drug column. Gather indices for reviews to keep.\n",
    "grouped_df_indices_to_keep = []\n",
    "\n",
    "# Find if the review column contains the string from the drug column.\n",
    "for row in tqdm(range(len(grouped_df.index))):\n",
    "    if (grouped_df.index[row][1].lower() in grouped_df.index[row][0].lower()) == True:\n",
    "        grouped_df_indices_to_keep.append(row)\n",
    "        \n",
    "grouped_df_indices_to_keep[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee119907",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grouped_df_indices_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems many rows should be kept. Check that this worked correctly.\n",
    "grouped_df.index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca439af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The drug name is in the review narrative. \n",
    "# Isolate just the rows to keep\n",
    "grouped_to_keep = pd.MultiIndex.to_frame(grouped_df.index[grouped_df_indices_to_keep])\n",
    "grouped_to_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54183088",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_to_keep = grouped_to_keep.reset_index(drop=True)\n",
    "grouped_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the correct number of rows for reviews that contain the drug name\n",
    "# Add the keep row so that this df can be merged with the original long_df\n",
    "grouped_to_keep['keep'] = 'yes'\n",
    "grouped_to_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3845ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = long_df.merge(right=grouped_to_keep, how='left', on=['review', 'drug'])\n",
    "long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c83fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This contains the correct number of rows to match the original long_df\n",
    "# keep_y has the values I need for knowing which rows to keep so far\n",
    "\n",
    "long_df = long_df.drop(columns=['keep_x'])\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = long_df.rename(columns={'keep_y':'keep'})\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d8909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na in keep column to make it easier to work with later.\n",
    "long_df['keep'] = long_df.keep.fillna('z')\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db02ed",
   "metadata": {},
   "source": [
    "Dig further into rows where the name of the drug is not in the review. This does not necessarily mean the review isn't applicable to the associated drug. But, I'd say that if there is a review that contains a drug name, that same review should be dropped wherever it appears along with a different drug not mentioned. \n",
    "\n",
    "<font color='violet'> Drop rows where text doesn't contain drug name but drug name is present in the same review for a different drug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d718553",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_drug_in_review = long_df.groupby(['review', 'keep']).count().sort_values(\n",
    "    by=['review', 'keep'])\n",
    "no_drug_in_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c9b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_drug_in_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caddcafc",
   "metadata": {},
   "source": [
    "There are fewer indices this time because some rows have multiple drugs aggregated within the 'z' row for a review. If a review has only unknown (z) keep values, that should remain unknown for now. But if there is a yes row for the review, then that review's z's should be come no's. \n",
    "\n",
    "Specifically, identify reviews for rows to keep. Then, since yes comes before z in the sorting, the yes row is on top in each set of rows per review. So, the row directly below each yes row can be deleted, IF it has the same review. (If it doesn't have the same review, then it should remain unknown for now). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_drop = []\n",
    "\n",
    "for idx in tqdm(range(len(no_drug_in_review))):\n",
    "    # Isolate reviews for rows to keep, and if  \n",
    "    if (no_drug_in_review.index[idx][1] == 'yes' and no_drug_in_review.index[idx][0] == \n",
    "        no_drug_in_review.index[idx+1][0]):\n",
    "        indices_to_drop.append(idx+1)\n",
    "\n",
    "indices_to_drop[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120da413",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indices_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ed093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm this worked correctly\n",
    "no_drug_in_review.index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_drug_in_review.index[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c13b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This worked correctly. Index 2 is slotted for dropping, and it has the same review as \n",
    "# index 1, which is labeled yes to keep. Now, isolate the rows to drop.\n",
    "\n",
    "un_reviewed_to_drop = pd.MultiIndex.to_frame(no_drug_in_review.index[indices_to_drop])\n",
    "un_reviewed_to_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1055a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_reviewed_to_drop = un_reviewed_to_drop.reset_index(drop=True)\n",
    "un_reviewed_to_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change keep value to no\n",
    "un_reviewed_to_drop['keep'] = 'no'\n",
    "un_reviewed_to_drop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d7c94",
   "metadata": {},
   "source": [
    "This can again be merged with long_df. There may be multiple drugs per \"no keep\" review, and that's okay; each one can be filled with no because these reviews should be dropped wherever they appear, since they already have an associated yes review that is definitely relevant to its associated drug. Wherever the new keep column says no but the old keep column says yes, the value should be yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = long_df.merge(right=un_reviewed_to_drop, on='review', how='left')\n",
    "long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, if keep_x = yes, that's the row to keep for that review. \n",
    "# anyplace where keep_x = z but keep_y = no, the keep value should end up as no\n",
    "\n",
    "for row in tqdm(range(len(long_df))):\n",
    "    if long_df.loc[row,'keep_y'] == 'no' and long_df.loc[row,'keep_x'] == 'z':\n",
    "        long_df.loc[row,'keep_x'] = 'no'\n",
    "\n",
    "long_df[long_df.keep_y=='no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b43cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if this worked correctly\n",
    "long_df[long_df.review == long_df.loc[122,'review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ec634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks correct. The drug name is in the review associated with the yes row\n",
    "# The matching review now says no in keep_x. I can delete the row keep_y\n",
    "\n",
    "long_df = long_df.drop(columns=['keep_y'])\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98162a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = long_df.rename(columns={'keep_x':'keep'})\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c119b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What remains? How many rows still have a keep value of z?\n",
    "len(long_df[long_df.keep=='z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476df06",
   "metadata": {},
   "source": [
    "<font color='violet'> Deal with any reviews that are just duplicates related to multiple conditions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40116423",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_condition = long_df.groupby(['review', 'condition']).count()\n",
    "grouped_by_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8768dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those duplicated by condition would show up where 2 subsequent indices have the same review.\n",
    "indices_duplicated_by_condition = []\n",
    "for idx in tqdm(range(len(grouped_by_condition))):\n",
    "    # Need to include a try-except since sometimes idx+1 won't exist\n",
    "    try:\n",
    "        if grouped_by_condition.index[idx][0] == grouped_by_condition.index[idx+1][0]:\n",
    "            indices_duplicated_by_condition.append(idx)\n",
    "            indices_duplicated_by_condition.append(idx+1)\n",
    "    except: pass\n",
    "        \n",
    "indices_duplicated_by_condition[:5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the rows I've identified\n",
    "duplicated_by_condition = pd.MultiIndex.to_frame(grouped_by_condition.index[\n",
    "    indices_duplicated_by_condition])\n",
    "duplicated_by_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d1a74",
   "metadata": {},
   "source": [
    "Here, I think it would make sense to just choose one of the conditions to keep. If there were many pairs like this, I might create columns \"condition1\" and \"condition2\", but if \"condition2\" would only have 4 values out of tens of thousands of rows, that seems like a waste. Instead, I'll go ahead and just keep the row for the less-common condition, so as to balance rather than further un-balance the condition column. \n",
    "\n",
    "First I'll need a dictionary of conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_rank = long_df.condition.value_counts().to_frame()\n",
    "conditions_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd25695",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_rank['rank'] = range(len(conditions_rank))\n",
    "conditions_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2dac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_rank = conditions_rank.drop(columns=['condition']).reset_index()\n",
    "conditions_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f922fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_rank = conditions_rank.rename(columns={'index':'condition'})\n",
    "conditions_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_rank = conditions_rank.set_index('condition').to_dict()['rank']\n",
    "conditions_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a9c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframe of just reviews that have multiple conditions attached\n",
    "duplicated_by_condition = duplicated_by_condition.reset_index(drop=True)\n",
    "duplicated_by_condition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get this in a format where the conditions for each review can be compared\n",
    "for row in range(len(duplicated_by_condition)):\n",
    "    duplicated_by_condition.loc[row,'rank'] = conditions_rank[duplicated_by_condition.loc[\n",
    "        row, 'condition']]\n",
    "\n",
    "duplicated_by_condition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify max rank as the condition to keep for each review\n",
    "condition_to_keep = duplicated_by_condition.groupby(['review']).max()\n",
    "condition_to_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b1044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the wrong condition listed, but the correct condition rank that should be kept.\n",
    "\n",
    "condition_to_keep = condition_to_keep.drop(columns=['condition'])\n",
    "condition_to_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change rank to int type\n",
    "condition_to_keep['rank'] = condition_to_keep['rank'].astype(int)\n",
    "condition_to_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d36889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regular df to iterate through:\n",
    "condition_to_keep = condition_to_keep.reset_index()\n",
    "condition_to_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2179453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refill conditions \n",
    "for row in range(len(condition_to_keep)):\n",
    "    for key, value in conditions_rank.items():\n",
    "        if condition_to_keep.loc[row,'rank'] == value:\n",
    "                condition_to_keep.loc[row,'condition'] = key\n",
    "            \n",
    "condition_to_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63675d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These conditions should have a keep value of 'yes'\n",
    "condition_to_keep['keep'] = 'yes'\n",
    "condition_to_keep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b436e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with duplicated_by_condition so as to be able to mark remaining rows with \"no\"\n",
    "duplicated_by_condition = duplicated_by_condition.merge(condition_to_keep, how='left')\n",
    "duplicated_by_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_by_condition = duplicated_by_condition.drop(columns=['rank']).fillna('no')\n",
    "duplicated_by_condition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ecbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now duplicated_by_condition can be merged with the rest of the long_df\n",
    "long_df = long_df.merge(duplicated_by_condition, on=['review', 'condition'], how='left')\n",
    "long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad295bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did that work? What does the first review with duplicated conditions look like?\n",
    "long_df[long_df.review.str.contains('After many months spent being given ten')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'd previously mis-labeled some rows. \n",
    "long_df.sort_values(by=['keep_y', 'keep_x']).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89895b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wherever keep_y is not null, that is the value that should be kept. \n",
    "# Otherwise keep the value of keep_y\n",
    "\n",
    "long_df = long_df.reset_index(drop = True)\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tqdm(range(len(long_df))):\n",
    "    if long_df.loc[row,'keep_y'] == 'yes' or long_df.loc[row,'keep_y'] == 'no':\n",
    "        long_df.loc[row,'keep'] = long_df.loc[row,'keep_y']\n",
    "    else: long_df.loc[row,'keep'] = long_df.loc[row,'keep_x']\n",
    "        \n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.sort_values(by=['keep_y', 'keep_x']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e04bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks correct so far. Clean up. \n",
    "long_df = long_df.drop(columns=['keep_x', 'keep_y'])\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b1053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f430506",
   "metadata": {},
   "source": [
    "Now, everywhere there is a duplicated review, a row for that review is being kept if it contains the drug name and it is submitted for the least-common condition. Reviews are marked for removal if they don't contain the name of the drug but their duplicate does. And being removed if submitted for a more-common condition where the review is also submitted for a less-common condition. \n",
    "\n",
    "But, wherever there is no drug name at all in the review, duplicates likely still exist across multiple drugs. This may be a place where new columns for drug1, drug2, drug3 may be necessary\n",
    "\n",
    "<font color='violet'> Deal with any remaining reviews duplicated across multiple drugs. But before going any further, mark all non-duplicated reviews to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.loc[(long_df.review.duplicated(keep=False)==False),'keep'] = 'yes'\n",
    "long_df[long_df.review.duplicated(keep=False)==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many reviews remain to deal with?\n",
    "len(long_df[(long_df.review.duplicated(keep=False)==True) & (long_df.keep=='z')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe67980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the highest number of drugs associated with a single review?\n",
    "row_count = long_df.groupby(['review']).count()\n",
    "row_count.sort_values(by='drug', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8344c8a1",
   "metadata": {},
   "source": [
    "The review \"Good\" is associated with 24 different drugs. Add columns drug0...drug23 wherever a review has more than one associated drug. First, sort drugs by prevalance, then enumerate drugs per review so that column can then become multiple nuew columns. Finally, create a pivot table and fill values of new drug_n columns with drug names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769af516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back and sort drugs according to how common they are so they're enumerated that way\n",
    "by_drug = long_df.groupby('drug').count().sort_values(by='rating', ascending=False)\n",
    "by_drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd1868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_drug['drug_prevalance'] = range(len(by_drug))\n",
    "by_drug = by_drug.drop(columns=[\n",
    "    'rating', 'condition', 'review', 'date', 'keep', 'drug_n']).reset_index()\n",
    "by_drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with long_df so that drugs have their prevalance values associated\n",
    "long_df = long_df.merge(by_drug, how='left')\n",
    "long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678fad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create drug_n to enumerate drugs per review\n",
    "long_df['drug_n'] = long_df.sort_values(by='drug_prevalance').groupby(['review']).cumcount()\n",
    "long_df.sort_values(by=['review', 'drug_n'])\n",
    "long_df.drug_n.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That appears to have worked. drug_n should contain values 0:23, for max 24 duplicates/review\n",
    "# Now fill in values for some new drug_n columns\n",
    "wid_df = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd26558",
   "metadata": {},
   "source": [
    "<font color='violet'> Lemmatize text and do further NLP & EDA in a new notebook once this first round of basic text cleaning is complete. \n",
    "    \n",
    "Resources with tips for effective EDA visualization with NLP:\n",
    "\n",
    "https://medium.com/plotly/nlp-visualisations-for-clear-immediate-insights-into-text-data-and-outputs-9ebfab168d5b\n",
    "    \n",
    "https://www.numpyninja.com/post/nlp-text-data-visualization\n",
    "    \n",
    "https://www.kaggle.com/code/sainathkrothapalli/nlp-visualisation-guide\n",
    "    \n",
    "https://medium.com/acing-ai/visualizations-in-natural-language-processing-2ca60dd34ce\n",
    "    \n",
    "https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n",
    "    \n",
    "https://towardsdatascience.com/getting-started-with-text-nlp-visualization-9dcb54bc91dd\n",
    "    \n",
    "https://www.kaggle.com/code/mitramir5/nlp-visualization-eda-glove\n",
    "    \n",
    "https://medium.com/analytics-vidhya/how-to-begin-performing-eda-on-nlp-ffdef92bedf6\n",
    "    \n",
    "https://inside-machinelearning.com/en/eda-nlp/\n",
    "    \n",
    "https://towardsdatascience.com/fundamental-eda-techniques-for-nlp-f81a93696a75\n",
    "    \n",
    "https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools\n",
    "    \n",
    "https://www.kdnuggets.com/2019/05/complete-exploratory-data-analysis-visualization-text-data.html\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c185b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
